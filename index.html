<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="PPEA-Depth Project Page">

  <title>PPEA-Depth: Progressive Parameter-Efficient Adaptation&#10 for Self-Supervised Monocular Depth Estimation</title>

  <link href="./static/css/bootstrap.min.css" rel="stylesheet">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <script src="./static/js/index.js"></script>

  <link href="./static/css/font.css" rel="stylesheet" type="text/css">
  <link href="./static/css/style.css" rel="stylesheet" type="text/css">

</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <!-- <div class="logo">
      <a href="https://mmlab.ie.cuhk.edu.hk/" target="_blank"><img src="./common/cuhk.jfif"></a>
    </div> -->
    <div class="title", style="font-size: 24pt; padding-top: 10pt;padding-left: 1pt">  <!-- Set padding as 10 if title is with two lines. -->
      PPEA-Depth: Progressive Parameter-Efficient Adaptation&#10 for Self-Supervised Monocular Depth Estimation
		<!-- <br>
		<font color="grey" size="4">arXiv Preprint.</font> -->
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a>Yue-Jiang Dong</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
    <a>Yuan-Chen Guo</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
    <a>Ying-Tian Liu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
    <a>Fang-Lue Zhang</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
    <a>Song-Hai Zhang</a><sup>1</sup>
  </div>
  <div class="institution">
    <sup>1</sup>Tsinghua University&nbsp;&nbsp;&nbsp;
    <sup>2</sup>Victoria University of Wellington
  </div>

  <div class="column has-text-centered">
    <div class="publication-links">
      <!-- PDF Link. -->
      <span class="link-block">
        <a href="./content/ppeadepth.pdf" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fas fa-file-pdf"></i>
          </span>
          <span>Paper</span>
        </a>
      </span>
      <span class="link-block">
        <a href="./content/supplement.pdf" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fas fa-file-pdf"></i>
          </span>
          <span>Appendix</span>
        </a>
      </span>

      <span class="link-block">
        <a href="https://arxiv.org/abs/2312.13066" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="ai ai-arxiv"></i>
          </span>
          <span>arXiv</span>
        </a>
      </span>
      
      <!-- Code Link. -->
      <span class="link-block">
        <a href="https://github.com/YuejiangDong/PPEA-Depth"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-github"></i>
          </span>
          <span>Github</span>
        </a>
      </span>
      <!-- Dataset Link.
      <span class="link-block">
        <a href="./#BibTeX" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fa fa-quote-left"></i>
          </span>
          <span>Cite</span>
        </a> -->
    </div>

  </div>
</div>
  <!-- <div class="link">
    <a href="https://arxiv.org/pdf/2212.02350.pdf" target="_blank">[Paper]</a>&nbsp;
    <a href="https://github.com/alvinliu0/ANGIE" target="_blank">[Code]</a>&nbsp;
    <a href="https://github.com/alvinliu0/ANGIE" target="_blank">[Dataset]</a>
  </div> -->

</div>
<!-- === Home Section Ends === -->

<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Abstract</div>
  <div class="body">
    Self-supervised monocular depth estimation is of significant importance with applications spanning across autonomous driving and robotics. However, the reliance on self-supervision introduces a strong static-scene assumption, thereby posing challenges in achieving optimal performance in dynamic scenes, which are prevalent in most real-world situations.
    
    To address these issues, we propose <i>PPEA-Depth</i>, a Progressive Parameter-Efficient Adaptation approach to transfer a pre-trained image model for self-supervised depth estimation. The training comprises two sequential stages: an initial phase trained on a dataset primarily composed of static scenes, succeeded by an expansion to more intricate datasets involving dynamic scenes. To facilitate this process, we design compact encoder and decoder adapters to enable parameter-efficient tuning, allowing the network to adapt effectively. They not only uphold generalized patterns from pre-trained image models but also retains knowledge gained from the preceding phase into the subsequent one. Extensive experiments demonstrate that PPEA-Depth achieves state-of-the-art performance on KITTI, CityScapes and DDAD datasets.
  </div>
  <div class="card-image" style="text-align: center;">
    <img src="./content/teaser.pdf" width="50%">
  </div>

  <i>Top:</i> The conventional training approach employs a consistent process for both static and dynamic datasets: it includes using a pre-trained image model as an encoder and fine-tuning all U-Net parameters for each dataset. 
  
  <i>Bottom:</i> Our novel two-stage training paradigm integrates adapters to progressively tailor the pre-trained image models for depth perception initially on simple dataset (static scenes primarily) and then extends to intricate datasets (with dynamic scenes).
</div>
<!-- === Overview Section Ends === -->

<!-- === Qualitative Result Section Starts === -->

<div class="section">
  <div class="title">Qualitative Results</div>
  <div style="text-align:center;">
    (Test Images are From CityScapes)
  </div>
  <div class="card-image" style="text-align: center;">
    <video id="qualitative" autoplay controls muted loop playsinline height="100%">
      <source src="./content/demo.mp4" type="video/mp4">
    </video>
  </div>
  <div class="body">
    <i>From Left to Right:</i> the original input image, the estimated depth obtained by full fine-tuning a U-Net from scratch, and the estimated depth produced by our PPEA-Depth approach.
    <br/><br/>
  </div>

</div>
<!-- === Overview Section Ends === -->



</body>
</html>